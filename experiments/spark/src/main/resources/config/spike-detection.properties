#sd.kafka.zookeeper.host=localhost:9092
#sd.kafka.spout.topic=sensors
#sd.kafka.zookeeper.path=/kafkastorm
#sd.kafka.consumer.id=kafkastormconsumer

sd.batchspout.class=spark.applications.spout.BatchMemFileSpout
sd.spout.class=spark.applications.spout.MemFileSpout
; sd.spout.class=storm.applications.spout.GeneratorSpout
sd.spout.path=/media/tony/ProfilingData/TestingData/data/app/sd/sensors.dat
sd.spout.parser=spark.applications.spout.parser.SensorParser
; sd.spout.generator=storm.applications.spout.generator.SensorGenerator

sd.spout.threads=1
sd.sink.threads=1
; sd.sink.class=storm.applications.sink.ConsoleSink
sd.sink.class=spark.applications.sink.NullSink
#sd.sink.path
#sd.sink.formatter
#sd.sink.socket.port
#sd.sink.socket.charset

sd.parser.value_field=temp
#sd.generator.count
sd.moving_average.window=1000

##LARGE PAGE CONFIG:
; sd.moving_average.threads=2
; sd.spike_detector.threads=1
; topology.acker.executors=2

; ##TUNED CONFIG (1 or 4 sockes):
sd.moving_average.threads=2
sd.spike_detector.threads=1
topology.acker.executors=2

##Batch fixed
; sd.moving_average.threads=1
##Batch Optimal::
; sd.moving_average.threads=1
; sd.spike_detector.threads=2
; topology.acker.executors=2

; sd.moving_average.threads=2
; sd.spike_detector.threads=1
; topology.acker.executors=2

sd.spike_detector.threshold=0.03

; metrics.enabled=true
; metrics.reporter=csv
; metrics.interval.value=1
; metrics.interval.unit=seconds
;total input size= 9242040
; n=4
end_index=8223600
max_pending=2313152